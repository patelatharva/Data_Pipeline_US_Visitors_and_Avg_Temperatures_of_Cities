{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline for creating data model to facilitate analysis of relation between termperatures around the cities of USA to the number and types of visitors of USA\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "I prepared data pipeline to create tables that enable analysis of number and types of visitors to various cities of USA around the year and how does it relate to the historical average temperatures for cities in USA around the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql.types import DateType, FloatType, LongType, DoubleType, StringType, IntegerType\n",
    "import re\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope \n",
    "I used data about visitors at different airports of USA along with historical records of temperature around year for cities in USA. I also used data about airport locations and demographics of cities across USA. I created data pipeline that extracted data from the downloaded datasets, performed necessary filtering and transformations, loaded it into different tables as per data model and saved in Parquet format with appropriate partitions using Apache Spark.\n",
    "\n",
    "#### Description of Data\n",
    "For this purpose, I collected data from following sources:\n",
    "* Data about the visitors arriving to different US airports in 2016 was retrieved from this site: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "* Data of average daily temperature for cities around the world from 1750 to 2013 was availed from this source: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "* Data about airport codes and their location was retrieved from this site: https://datahub.io/core/airport-codes\n",
    "* Data about demographics of various cities based on U.S. Census data of 2015 from this source: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data = spark.read.format(\"csv\").option(\"header\", True).load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "temp_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap_data = spark.read.format(\"csv\").option(\"header\", True).load(\"airport-codes_csv.csv\")\n",
    "ap_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_data = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "dem_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of all the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Airports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap_data = spark.read.format(\"csv\").option(\"header\", True).load(\"airport-codes_csv.csv\")\n",
    "ap_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The airports data includes details about all the airports around the world including those that do not have IATA code assigned to them. Based on the scope of the project, I decided to process only those airports for which IATA code is present in the row and are located in United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|iata_code| municipality|\n",
      "+---------+-------------+\n",
      "|      OCA|    Key Largo|\n",
      "|      PQS|Pilot Station|\n",
      "|      CSE|Crested Butte|\n",
      "|      JCY| Johnson City|\n",
      "|      PMX|       Palmer|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_ap_data = ap_data.filter(ap_data[\"iso_country\"] == \"US\").filter(ap_data[\"iata_code\"] != \"null\").selectExpr(\"iata_code\", \"municipality\")\n",
    "us_ap_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing temperatures data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data = spark.read.format(\"csv\").option(\"header\", True).load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "temp_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I noticed that some rows have `null` values for AverageTemperature and AverageTemperatureUncertainty. As the file original file is in CSV format, all the columns are represented as `String` in the dataframe.\n",
    "* `dt` column contains date in string fomat, that needs to be converted to DateType\n",
    "* `AverageTemperature` and `AverageTemperatureUncertainty` need to be converted from string to FloatType\n",
    "Here is how this data can be processed during transformation stage of pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------+---------+---------------+---------------------------+\n",
      "| city|country|latitude|longitude|avg_temperature|avg_temperature_uncertainty|\n",
      "+-----+-------+--------+---------+---------------+---------------------------+\n",
      "|Århus|Denmark|  57.05N|   10.33E|          6.068|                      1.737|\n",
      "|Århus|Denmark|  57.05N|   10.33E|          5.788|                      3.624|\n",
      "|Århus|Denmark|  57.05N|   10.33E|         10.644|                      1.283|\n",
      "|Århus|Denmark|  57.05N|   10.33E|         14.051|                      1.347|\n",
      "|Århus|Denmark|  57.05N|   10.33E|         16.082|                      1.396|\n",
      "+-----+-------+--------+---------+---------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- avg_temperature: float (nullable = true)\n",
      " |-- avg_temperature_uncertainty: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_datetime(date_string):\n",
    "    return dt.strptime(date_string, '%Y-%m-%d')\n",
    "udf_datetime = udf(lambda date_string: get_datetime(date_string), DateType())\n",
    "# Filtering out rows with null value in AverageTemperature\n",
    "# Adding date column of DateType\n",
    "# Converting AverageTemperature and AverageTemperatureUncertainty into Float format\n",
    "clean_temp_data = temp_data.where(temp_data[\"AverageTemperature\"].isNotNull()) \\\n",
    "                            .withColumn(\"date\", udf_datetime(temp_data.dt)) \\\n",
    "                            .selectExpr([\"City as city\", \n",
    "                                         \"Country as country\", \n",
    "                                         \"Latitude as latitude\", \n",
    "                                         \"Longitude as longitude\", \n",
    "                                         \"cast(AverageTemperature as float) as avg_temperature\",\n",
    "                                         \"cast(AverageTemperatureUncertainty as float) as avg_temperature_uncertainty\"])\n",
    "clean_temp_data.show(5)\n",
    "clean_temp_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing US demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_data.printSchema()\n",
    "dem_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demographics data contains various numeric fields like population counts of different genders as well as median age represented as `String` format in dataframe.\n",
    "* Fields `Male Population`, 'Female Population`, `Total Population`, `Foreign-born` needs to be converted to Long data type.\n",
    "* `Median Age` needs to be converted to Float data type.\n",
    "* `Average Household Size` can be converted to Integer data type.  \n",
    "\n",
    "I have shown below, how this data can be processed during transformation stage of pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- male_population: long (nullable = true)\n",
      " |-- female_population: long (nullable = true)\n",
      " |-- total_population: long (nullable = true)\n",
      " |-- foreign_born: long (nullable = true)\n",
      " |-- avg_household_size: integer (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+\n",
      "|            city|        state|state_code|male_population|female_population|total_population|foreign_born|avg_household_size|median_age|                race|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+\n",
      "|   Silver Spring|     Maryland|        MD|          40601|            41862|           82463|       30908|                 2|      33.8|  Hispanic or Latino|\n",
      "|          Quincy|Massachusetts|        MA|          44129|            49500|           93629|       32935|                 2|      41.0|               White|\n",
      "|          Hoover|      Alabama|        AL|          38040|            46799|           84839|        8229|                 2|      38.5|               Asian|\n",
      "|Rancho Cucamonga|   California|        CA|          88127|            87105|          175232|       33878|                 3|      34.5|Black or African-...|\n",
      "|          Newark|   New Jersey|        NJ|         138040|           143873|          281913|       86253|                 2|      34.6|               White|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_dem_data = dem_data.withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                    .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "                    .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "                    .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "                    .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "                    .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "                    .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "                    .selectExpr(\"City as city\", \n",
    "                        \"State as state\", \n",
    "                        \"state_code\",\n",
    "                        \"cast(male_population as bigint) as male_population\", \n",
    "                        \"cast(female_population as bigint) as female_population\", \n",
    "                        \"cast(total_population as bigint) as total_population\", \n",
    "                        \"cast(foreign_born as bigint) as foreign_born\",\n",
    "                        \"cast(avg_household_size as int) as avg_household_size\", \n",
    "                        \"cast(median_age as float) as median_age\", \n",
    "                        \"Race as race\")\n",
    "valid_dem_data.printSchema()\n",
    "valid_dem_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.printSchema()\n",
    "immigration_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.filter(immigration_data[\"i94port\"] == \"XXX\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many rows in the data set with XXX value assigned to `i94port` column. XXX code is not assigned to any airport by IATA. That means that this value is used as placeholder for indicating an airport which does not have IATA code assigned to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.where(F.isnull(F.col(\"i94addr\"))).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also many rows that have null value for `i94addr` which indicates two letter abbrevation code for the first state to be visited by the visitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.where(F.isnull(F.col(\"visatype\"))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|visatype|\n",
      "+--------+\n",
      "|      F2|\n",
      "|     GMB|\n",
      "|      B2|\n",
      "|      F1|\n",
      "|     CPL|\n",
      "|      I1|\n",
      "|      WB|\n",
      "|      M1|\n",
      "|      B1|\n",
      "|      WT|\n",
      "|      M2|\n",
      "|      CP|\n",
      "|     GMT|\n",
      "|      E1|\n",
      "|       I|\n",
      "|      E2|\n",
      "|     SBP|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.selectExpr(\"visatype\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `i94cit` indicate the country of residence where the visitor is coming from. In SAS file, it is represented as some numeric code whose mapping to country names is provided in the label description file original data source. The codes can be replaced by the country name and added as new column named `country_of_residence` to dataframe.\n",
    "* The `arrdate` indiating the date of arrival of visitor is in SAS encoding of date which is essentially number of days after 1st June 1960 on which the targeted date comes. This can be converted to Date type in Spark and added as column `arrival_date` to dataframe.\n",
    "* `i94yr` indicating arrival year of visitor can be cast to Integer type and column renamed to `arrival_year'.\n",
    "* `i94mon` indicating arrival year of visitor can be cast to Integer type and column renamed to `arrival_month`.\n",
    "* `biryear` indicating year of birth of visitor can be cast to Integer type and column renamed to `birth_year`.\n",
    "* `i94bir` indicating age of visitor can be cast to Integer type and column renamed to `age`.\n",
    "* `i94port` indicating IATA code of airport where the user arrived can be renamed to `iata_code`.\n",
    "\n",
    "The above cleaning and transformation can be done using code shown below during transformation step of pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------+---------+-------------------+----------+---+--------------------+\n",
      "|arrival_date|arrival_year|arrival_month|iata_code|first_state_visited|birth_year|age|country_of_residence|\n",
      "+------------+------------+-------------+---------+-------------------+----------+---+--------------------+\n",
      "|  2016-04-29|        2016|            4|      XXX|               null|      1979| 37|             ECUADOR|\n",
      "|  2016-04-07|        2016|            4|      ATL|                 AL|      1991| 25|         SOUTH KOREA|\n",
      "|  2016-04-01|        2016|            4|      WAS|                 MI|      1961| 55|             ALBANIA|\n",
      "|  2016-04-01|        2016|            4|      NYC|                 MA|      1988| 28|             ALBANIA|\n",
      "|  2016-04-01|        2016|            4|      NYC|                 MA|      2012|  4|             ALBANIA|\n",
      "+------------+------------+-------------+---------+-------------------+----------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- first_state_visited: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country_of_residence: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_date(sas_numeric_date):\n",
    "    date = pd.to_timedelta(sas_numeric_date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    return date\n",
    "udf_date = udf(lambda x : get_date(x), DateType())\n",
    "code_to_country_string = \"\"\"\n",
    " 582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\n",
    "   236 =  'AFGHANISTAN'\n",
    "   101 =  'ALBANIA'\n",
    "   316 =  'ALGERIA'\n",
    "   102 =  'ANDORRA'\n",
    "   324 =  'ANGOLA'\n",
    "   529 =  'ANGUILLA'\n",
    "   518 =  'ANTIGUA-BARBUDA'\n",
    "   687 =  'ARGENTINA '\n",
    "   151 =  'ARMENIA'\n",
    "   532 =  'ARUBA'\n",
    "   438 =  'AUSTRALIA'\n",
    "   103 =  'AUSTRIA'\n",
    "   152 =  'AZERBAIJAN'\n",
    "   512 =  'BAHAMAS'\n",
    "   298 =  'BAHRAIN'\n",
    "   274 =  'BANGLADESH'\n",
    "   513 =  'BARBADOS'\n",
    "   104 =  'BELGIUM'\n",
    "   581 =  'BELIZE'\n",
    "   386 =  'BENIN'\n",
    "   509 =  'BERMUDA'\n",
    "   153 =  'BELARUS'\n",
    "   242 =  'BHUTAN'\n",
    "   688 =  'BOLIVIA'\n",
    "   717 =  'BONAIRE, ST EUSTATIUS, SABA' \n",
    "   164 =  'BOSNIA-HERZEGOVINA'\n",
    "   336 =  'BOTSWANA'\n",
    "   689 =  'BRAZIL'\n",
    "   525 =  'BRITISH VIRGIN ISLANDS'\n",
    "   217 =  'BRUNEI'\n",
    "   105 =  'BULGARIA'\n",
    "   393 =  'BURKINA FASO'\n",
    "   243 =  'BURMA'\n",
    "   375 =  'BURUNDI'\n",
    "   310 =  'CAMEROON'\n",
    "   326 =  'CAPE VERDE'\n",
    "   526 =  'CAYMAN ISLANDS'\n",
    "   383 =  'CENTRAL AFRICAN REPUBLIC'\n",
    "   384 =  'CHAD'\n",
    "   690 =  'CHILE'\n",
    "   245 =  'CHINA, PRC'\n",
    "   721 =  'CURACAO' \n",
    "   270 =  'CHRISTMAS ISLAND'\n",
    "   271 =  'COCOS ISLANDS'\n",
    "   691 =  'COLOMBIA'\n",
    "   317 =  'COMOROS'\n",
    "   385 =  'CONGO'\n",
    "   467 =  'COOK ISLANDS'\n",
    "   575 =  'COSTA RICA'\n",
    "   165 =  'CROATIA'\n",
    "   584 =  'CUBA'\n",
    "   218 =  'CYPRUS'\n",
    "   140 =  'CZECH REPUBLIC'\n",
    "   723 =  'FAROE ISLANDS (PART OF DENMARK)'  \n",
    "   108 =  'DENMARK'\n",
    "   322 =  'DJIBOUTI'\n",
    "   519 =  'DOMINICA'\n",
    "   585 =  'DOMINICAN REPUBLIC'\n",
    "   240 =  'EAST TIMOR'\n",
    "   692 =  'ECUADOR'\n",
    "   368 =  'EGYPT'\n",
    "   576 =  'EL SALVADOR'\n",
    "   399 =  'EQUATORIAL GUINEA'\n",
    "   372 =  'ERITREA'\n",
    "   109 =  'ESTONIA'\n",
    "   369 =  'ETHIOPIA'\n",
    "   604 =  'FALKLAND ISLANDS'\n",
    "   413 =  'FIJI'\n",
    "   110 =  'FINLAND'\n",
    "   111 =  'FRANCE'\n",
    "   601 =  'FRENCH GUIANA'\n",
    "   411 =  'FRENCH POLYNESIA'\n",
    "   387 =  'GABON'\n",
    "   338 =  'GAMBIA'\n",
    "   758 =  'GAZA STRIP' \n",
    "   154 =  'GEORGIA'\n",
    "   112 =  'GERMANY'\n",
    "   339 =  'GHANA'\n",
    "   143 =  'GIBRALTAR'\n",
    "   113 =  'GREECE'\n",
    "   520 =  'GRENADA'\n",
    "   507 =  'GUADELOUPE'\n",
    "   577 =  'GUATEMALA'\n",
    "   382 =  'GUINEA'\n",
    "   327 =  'GUINEA-BISSAU'\n",
    "   603 =  'GUYANA'\n",
    "   586 =  'HAITI'\n",
    "   726 =  'HEARD AND MCDONALD IS.'\n",
    "   149 =  'HOLY SEE/VATICAN'\n",
    "   528 =  'HONDURAS'\n",
    "   206 =  'HONG KONG'\n",
    "   114 =  'HUNGARY'\n",
    "   115 =  'ICELAND'\n",
    "   213 =  'INDIA'\n",
    "   759 =  'INDIAN OCEAN AREAS (FRENCH)' \n",
    "   729 =  'INDIAN OCEAN TERRITORY' \n",
    "   204 =  'INDONESIA'\n",
    "   249 =  'IRAN'\n",
    "   250 =  'IRAQ'\n",
    "   116 =  'IRELAND'\n",
    "   251 =  'ISRAEL'\n",
    "   117 =  'ITALY'\n",
    "   388 =  'IVORY COAST'\n",
    "   514 =  'JAMAICA'\n",
    "   209 =  'JAPAN'\n",
    "   253 =  'JORDAN'\n",
    "   201 =  'KAMPUCHEA'\n",
    "   155 =  'KAZAKHSTAN'\n",
    "   340 =  'KENYA'\n",
    "   414 =  'KIRIBATI'\n",
    "   732 =  'KOSOVO' \n",
    "   272 =  'KUWAIT'\n",
    "   156 =  'KYRGYZSTAN'\n",
    "   203 =  'LAOS'\n",
    "   118 =  'LATVIA'\n",
    "   255 =  'LEBANON'\n",
    "   335 =  'LESOTHO'\n",
    "   370 =  'LIBERIA'\n",
    "   381 =  'LIBYA'\n",
    "   119 =  'LIECHTENSTEIN'\n",
    "   120 =  'LITHUANIA'\n",
    "   121 =  'LUXEMBOURG'\n",
    "   214 =  'MACAU'\n",
    "   167 =  'MACEDONIA'\n",
    "   320 =  'MADAGASCAR'\n",
    "   345 =  'MALAWI'\n",
    "   273 =  'MALAYSIA'\n",
    "   220 =  'MALDIVES'\n",
    "   392 =  'MALI'\n",
    "   145 =  'MALTA'\n",
    "   472 =  'MARSHALL ISLANDS'\n",
    "   511 =  'MARTINIQUE'\n",
    "   389 =  'MAURITANIA'\n",
    "   342 =  'MAURITIUS'\n",
    "   760 =  'MAYOTTE (AFRICA - FRENCH)' \n",
    "   473 =  'MICRONESIA, FED. STATES OF'\n",
    "   157 =  'MOLDOVA'\n",
    "   122 =  'MONACO'\n",
    "   299 =  'MONGOLIA'\n",
    "   735 =  'MONTENEGRO' \n",
    "   521 =  'MONTSERRAT'\n",
    "   332 =  'MOROCCO'\n",
    "   329 =  'MOZAMBIQUE'\n",
    "   371 =  'NAMIBIA'\n",
    "   440 =  'NAURU'\n",
    "   257 =  'NEPAL'\n",
    "   123 =  'NETHERLANDS'\n",
    "   508 =  'NETHERLANDS ANTILLES'\n",
    "   409 =  'NEW CALEDONIA'\n",
    "   464 =  'NEW ZEALAND'\n",
    "   579 =  'NICARAGUA'\n",
    "   390 =  'NIGER'\n",
    "   343 =  'NIGERIA'\n",
    "   470 =  'NIUE'\n",
    "   275 =  'NORTH KOREA'\n",
    "   124 =  'NORWAY'\n",
    "   256 =  'OMAN'\n",
    "   258 =  'PAKISTAN'\n",
    "   474 =  'PALAU'\n",
    "   743 =  'PALESTINE' \n",
    "   504 =  'PANAMA'\n",
    "   441 =  'PAPUA NEW GUINEA'\n",
    "   693 =  'PARAGUAY'\n",
    "   694 =  'PERU'\n",
    "   260 =  'PHILIPPINES'\n",
    "   416 =  'PITCAIRN ISLANDS'\n",
    "   107 =  'POLAND'\n",
    "   126 =  'PORTUGAL'\n",
    "   297 =  'QATAR'\n",
    "   748 =  'REPUBLIC OF SOUTH SUDAN'\n",
    "   321 =  'REUNION'\n",
    "   127 =  'ROMANIA'\n",
    "   158 =  'RUSSIA'\n",
    "   376 =  'RWANDA'\n",
    "   128 =  'SAN MARINO'\n",
    "   330 =  'SAO TOME AND PRINCIPE'\n",
    "   261 =  'SAUDI ARABIA'\n",
    "   391 =  'SENEGAL'\n",
    "   142 =  'SERBIA AND MONTENEGRO'\n",
    "   745 =  'SERBIA' \n",
    "   347 =  'SEYCHELLES'\n",
    "   348 =  'SIERRA LEONE'\n",
    "   207 =  'SINGAPORE'\n",
    "   141 =  'SLOVAKIA'\n",
    "   166 =  'SLOVENIA'\n",
    "   412 =  'SOLOMON ISLANDS'\n",
    "   397 =  'SOMALIA'\n",
    "   373 =  'SOUTH AFRICA'\n",
    "   276 =  'SOUTH KOREA'\n",
    "   129 =  'SPAIN'\n",
    "   244 =  'SRI LANKA'\n",
    "   346 =  'ST. HELENA'\n",
    "   522 =  'ST. KITTS-NEVIS'\n",
    "   523 =  'ST. LUCIA'\n",
    "   502 =  'ST. PIERRE AND MIQUELON'\n",
    "   524 =  'ST. VINCENT-GRENADINES'\n",
    "   716 =  'SAINT BARTHELEMY' \n",
    "   736 =  'SAINT MARTIN' \n",
    "   749 =  'SAINT MAARTEN' \n",
    "   350 =  'SUDAN'\n",
    "   602 =  'SURINAME'\n",
    "   351 =  'SWAZILAND'\n",
    "   130 =  'SWEDEN'\n",
    "   131 =  'SWITZERLAND'\n",
    "   262 =  'SYRIA'\n",
    "   268 =  'TAIWAN'\n",
    "   159 =  'TAJIKISTAN'\n",
    "   353 =  'TANZANIA'\n",
    "   263 =  'THAILAND'\n",
    "   304 =  'TOGO'\n",
    "   417 =  'TONGA'\n",
    "   516 =  'TRINIDAD AND TOBAGO'\n",
    "   323 =  'TUNISIA'\n",
    "   264 =  'TURKEY'\n",
    "   161 =  'TURKMENISTAN'\n",
    "   527 =  'TURKS AND CAICOS ISLANDS'\n",
    "   420 =  'TUVALU'\n",
    "   352 =  'UGANDA'\n",
    "   162 =  'UKRAINE'\n",
    "   296 =  'UNITED ARAB EMIRATES'\n",
    "   135 =  'UNITED KINGDOM'\n",
    "   695 =  'URUGUAY'\n",
    "   163 =  'UZBEKISTAN'\n",
    "   410 =  'VANUATU'\n",
    "   696 =  'VENEZUELA'\n",
    "   266 =  'VIETNAM'\n",
    "   469 =  'WALLIS AND FUTUNA ISLANDS'\n",
    "   757 =  'WEST INDIES (FRENCH)' \n",
    "   333 =  'WESTERN SAHARA'\n",
    "   465 =  'WESTERN SAMOA'\n",
    "   216 =  'YEMEN'\n",
    "   139 =  'YUGOSLAVIA'\n",
    "   301 =  'ZAIRE'\n",
    "   344 =  'ZAMBIA'\n",
    "   315 =  'ZIMBABWE'\n",
    "   403 =  'INVALID: AMERICAN SAMOA'\n",
    "   712 =  'INVALID: ANTARCTICA' \n",
    "   700 =  'INVALID: BORN ON BOARD SHIP'\n",
    "   719 =  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)'\n",
    "   574 =  'INVALID: CANADA'\n",
    "   720 =  'INVALID: CANTON AND ENDERBURY ISLS' \n",
    "   106 =  'INVALID: CZECHOSLOVAKIA'\n",
    "   739 =  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' \n",
    "   394 =  'INVALID: FRENCH SOUTHERN AND ANTARCTIC'\n",
    "   501 =  'INVALID: GREENLAND'\n",
    "   404 =  'INVALID: GUAM'\n",
    "   730 =  'INVALID: INTERNATIONAL WATERS' \n",
    "   731 =  'INVALID: JOHNSON ISLAND' \n",
    "   471 =  'INVALID: MARIANA ISLANDS, NORTHERN'\n",
    "   737 =  'INVALID: MIDWAY ISLANDS' \n",
    "   753 =  'INVALID: MINOR OUTLYING ISLANDS - USA'\n",
    "   740 =  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' \n",
    "   710 =  'INVALID: NON-QUOTA IMMIGRANT'\n",
    "   505 =  'INVALID: PUERTO RICO'\n",
    "    0  =  'INVALID: STATELESS'\n",
    "   705 =  'INVALID: STATELESS'\n",
    "   583 =  'INVALID: UNITED STATES'\n",
    "   407 =  'INVALID: UNITED STATES'\n",
    "   999 =  'INVALID: UNKNOWN'\n",
    "   239 =  'INVALID: UNKNOWN COUNTRY'\n",
    "   134 =  'INVALID: USSR'\n",
    "   506 =  'INVALID: U.S. VIRGIN ISLANDS'\n",
    "   755 =  'INVALID: WAKE ISLAND'  \n",
    "   311 =  'Collapsed Tanzania (should not show)'\n",
    "   741 =  'Collapsed Curacao (should not show)'\n",
    "    54 =  'No Country Code (54)'\n",
    "   100 =  'No Country Code (100)'\n",
    "   187 =  'No Country Code (187)'\n",
    "   190 =  'No Country Code (190)'\n",
    "   200 =  'No Country Code (200)'\n",
    "   219 =  'No Country Code (219)'\n",
    "   238 =  'No Country Code (238)'\n",
    "   277 =  'No Country Code (277)'\n",
    "   293 =  'No Country Code (293)'\n",
    "   300 =  'No Country Code (300)'\n",
    "   319 =  'No Country Code (319)'\n",
    "   365 =  'No Country Code (365)'\n",
    "   395 =  'No Country Code (395)'\n",
    "   400 =  'No Country Code (400)'\n",
    "   485 =  'No Country Code (485)'\n",
    "   503 =  'No Country Code (503)'\n",
    "   589 =  'No Country Code (589)'\n",
    "   592 =  'No Country Code (592)'\n",
    "   791 =  'No Country Code (791)'\n",
    "   849 =  'No Country Code (849)'\n",
    "   914 =  'No Country Code (914)'\n",
    "   944 =  'No Country Code (944)'\n",
    "   996 =  'No Country Code (996)'\n",
    " \"\"\"\n",
    "code_to_country = {}\n",
    "for line in code_to_country_string.split(\"\\n\"):\n",
    "    line = line.strip()\n",
    "    match = re.search(r\"([\\d]+)[=\\s]+'([\\w.\\(\\)\\s:]+)'\", line)    \n",
    "    if match:\n",
    "        code = match.group(1)\n",
    "        country = match.group(2)\n",
    "        country = country.replace(\"INVALID: \", \"\")\n",
    "        code_to_country[float(code)] = country\n",
    "udf_code_to_country = udf(lambda code : code_to_country.get(code, None))        \n",
    "valid_migration_data = immigration_data.withColumn(\"arrival_date\", udf_date(immigration_data.arrdate)) \\\n",
    "                                        .withColumn(\"country_of_residence\", udf_code_to_country(immigration_data.i94res)) \\\n",
    "                                        .withColumnRenamed(\"i94port\", \"iata_code\") \\\n",
    "                                        .selectExpr(\"arrival_date\",\n",
    "                                                    \"cast(i94yr as int) as arrival_year\", \n",
    "                                                    \"cast(i94mon as int) as arrival_month\",                                                    \n",
    "                                                    \"iata_code\",\n",
    "                                                    \"i94addr as first_state_visited\",\n",
    "                                                    \"cast(biryear as int) as birth_year\",\n",
    "                                                    \"cast(i94bir as int) as age\",\n",
    "                                                    \"country_of_residence\")\n",
    "valid_migration_data.show(5)\n",
    "valid_migration_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model\n",
    "#### Conceptual Data Model\n",
    "I decided to represent this data in the schema with\n",
    "* Two Dimension tables called `airports_in_usa` and `us_demogrphics`.\n",
    "* Two Fact tables called `temperatures` and `us_visitors`\n",
    "\n",
    "The `airports_in_usa` will contain the information about airports with attributes:\n",
    "* iata_code: IATA code of airport\n",
    "* municipality: name of jurisdiction where the airport is located\n",
    "\n",
    "Based on the type of data to be stored in `airports_in_usa`, which is like containing labels or categorical data. It be easily considered to be a Dimension table.\n",
    "\n",
    "The `us_demographics` will contain the snapshot of US population in various cities at 2015 with attributes:\n",
    "* state: Name of state of USA\n",
    "* state_code: two letter abbreviated state code for the state\n",
    "* male_population: population count of males\n",
    "* female_population: population count of females\n",
    "* total_population: total population of the city\n",
    "* foreign_born: population count of foreign born residents\n",
    "* avg_household_size: average household size\n",
    "* median_age: median age of people\n",
    "* race: race of the people\n",
    "\n",
    "The `us_demographics` contains population count for various cities in USA. However, the Census is published on yearly basis. As many of the Census surveys are performed on annual basis of frequency, it is clear that the data in them is going to be updated less frequently. Therefore they can be thought as Dimension table.\n",
    "\n",
    "The `temperatures` table will contain information about averaget temperature of different cities around the world with following columns:\n",
    "* city (string): name of city\n",
    "* country (string): name of country\n",
    "* latitude (string): latitude of the city\n",
    "* longitude (string): longitude of the city\n",
    "* avg_temperature (float): average temperature of the day\n",
    "* avg_temperature_uncertainty (float): the 95% confidence interval around the average temperature\n",
    "* date (date): date when the reading of temperature was taken\n",
    "* month (integer): month of the temperature reading\n",
    "* year (integer): year of the temperature reading\n",
    "\n",
    "The data stored in `temperatures` table will look like a timeseries data with numeric values representing average temperature at cities around the world. It is possible to update this table regularly by appending rows to it when the daily average temperature readings for different cities around the world become available. Considering this, it can be considered as Fact table in this schema.  \n",
    "\n",
    "The `us_visitors` table will contain timeseries data of visitors arriving at different airports in the USA on each day around the year. The columns in table will be:\n",
    "* arrival_date: date of arrival of visitor\n",
    "* arriva_month: month of arrival of visitor\n",
    "* arrival_year: year of arrival of visitor\n",
    "* country_of_residence: country of residence of visitor\n",
    "* iata_code: IATA code of airport where the visitor arrived\n",
    "* first_state_visited: two letter abbreviation of state code to be first visited by visitor\n",
    "* birth_year: birth year of visitor\n",
    "* age: age of visitor on arrival to USA\n",
    "* visa_type: type of visa of visitor\n",
    "* airport_municipality: place where airport of arrival is located\n",
    "\n",
    "Currently it has data of 2016. It can be udpated by appending new rows to it for representing the data when the new data about visitor becomes available. This table can potentially contain millions of rows representing facts and therefore can be seen as a Fact table. Location of airport will be added in column `airport_municipality` to allow performing aggregation during analytical queries over that attribute as well as joining it with `temperatures` table while doing statistical analysis of average city temperatures with immigration patterns.\n",
    "\n",
    "#### Map of Data Pipelines\n",
    "Following are the steps to be performed in the data pipelines to create tables in the model described above.  \n",
    "1. Airports data is stored in CSV file. It will be read into a Spark dataframe and performed upon necessary transformations to prepare table `airports_in_usa` and load it as Parquet file format as a Dimension table in our data model.\n",
    "2. Temperatures data is stored in CSV file. It will be read into a Spark dataframe and performed upon necessary transformations to prepare table `temperatures` and load it as Parquet file format as a Fact table in our data model.\n",
    "3. US Cities demographcis data from 2015 is stored in CSV file. It will be read into a Spark dataframe and performed upon necessary transformations to prepare table `us_demographics` and load it as Parquet file format as a Dimension table in our data model.\n",
    "4. Immigration data is currently stored in separate files in SAS binary data file format with one file for each month of 2016. ETL process will extract data from each of these files by looping through the each month in the year, perform necessary transformations on data of that month and combine it with the data of earlier months into one table called `us_visitors` and load it as Parquet file format as a Fact table in our data model. The municipality where the airport of visitor's arrival is stored airports dataset and it will be added to this table as `airport_municipality` during the transformation stage of this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Pipelines to Model the Data \n",
    "#### Creating the data model\n",
    "Below is the code for methods that called to execute ETL steps on downloaded datasets to create final data tables as described in data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temperature_data():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).load(\"../../data2/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_temperature_data(temp_data):\n",
    "    def get_datetime(date_string):\n",
    "        return dt.strptime(date_string, '%Y-%m-%d')\n",
    "    udf_datetime = udf(lambda date_string: get_datetime(date_string), DateType())\n",
    "    # Filtering out rows with null value in AverageTemperature\n",
    "    # Adding date column of DateType\n",
    "    clean_temp_data = temp_data.where(temp_data[\"AverageTemperature\"].isNotNull()) \\\n",
    "                            .withColumn(\"date\", udf_datetime(temp_data.dt)) \\\n",
    "                            .selectExpr([\"City as city\", \n",
    "                                         \"Country as country\", \n",
    "                                         \"Latitude as latitude\", \n",
    "                                         \"Longitude as longitude\", \n",
    "                                             \"cast(AverageTemperature as float) as avg_temperature\",\n",
    "                                            \"cast(AverageTemperatureUncertainty as float) as avg_temperature_uncertainty\",\n",
    "                                            \"date\",\n",
    "                                            \"month(date) as month\",\n",
    "                                            \"year(date) as year\"]) \\\n",
    "                            .where(temp_data[\"AverageTemperature\"].isNotNull()) \n",
    "                                \n",
    "    return clean_temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temperature_data(df):\n",
    "    df.write.parquet(path=\"temperatures.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_raw = extract_temperature_data()\n",
    "transformed_data = transform_temperature_data(temp_data_raw)\n",
    "load_temperature_data(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_airports_data ():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).load(\"airport-codes_csv.csv\")\n",
    "def transform_airports_data (ap_data):\n",
    "    us_ap_data = ap_data.filter(ap_data[\"iso_country\"] == \"US\").filter(ap_data[\"iata_code\"] != \"null\").selectExpr(\"iata_code\", \"municipality\")\n",
    "    return us_ap_data\n",
    "def load_airports_data(us_ap_data):\n",
    "    us_ap_data.write.parquet(path=\"airports_in_usa.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_data_raw = extract_airports_data()\n",
    "us_airports_data = transform_airports_data(ap_data_raw)\n",
    "load_airports_data(us_airports_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_demographics_data():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "def transform_demographics_data(dem_data):\n",
    "    valid_dem_data = dem_data.withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                    .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "                    .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "                    .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "                    .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "                    .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "                    .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "                    .selectExpr(\"City as city\", \n",
    "                        \"State as state\", \n",
    "                        \"state_code\",\n",
    "                        \"cast(male_population as bigint) as male_population\", \n",
    "                        \"cast(female_population as bigint) as female_population\", \n",
    "                        \"cast(total_population as bigint) as total_population\",\n",
    "                        \"cast(foreign_born as bigint) as foreign_born\",\n",
    "                        \"cast(avg_household_size as int) as avg_household_size\", \n",
    "                        \"cast(median_age as float) as median_age\", \n",
    "                        \"Race as race\")\n",
    "    return valid_dem_data\n",
    "def load_demographics_data(dem_data):\n",
    "    dem_data.write.parquet(path=\"us_demographics.parquet\", partitionBy=[\"state_code\"], mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_data_raw = extract_demographics_data()\n",
    "valid_dem_data = transform_demographics_data(dem_data_raw)\n",
    "load_demographics_data(valid_dem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_immigration_data (month, year):\n",
    "    data = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_{}{}_sub.sas7bdat'.format(month, year))\n",
    "    return data\n",
    "\n",
    "def transform_migration_data(immigration_data):\n",
    "    def get_date(sas_numeric_date):\n",
    "        date = pd.to_timedelta(sas_numeric_date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "        return date\n",
    "    udf_date = udf(lambda x : get_date(x), DateType())\n",
    "    code_to_country_string = \"\"\"\n",
    "     582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\n",
    "       236 =  'AFGHANISTAN'\n",
    "       101 =  'ALBANIA'\n",
    "       316 =  'ALGERIA'\n",
    "       102 =  'ANDORRA'\n",
    "       324 =  'ANGOLA'\n",
    "       529 =  'ANGUILLA'\n",
    "       518 =  'ANTIGUA-BARBUDA'\n",
    "       687 =  'ARGENTINA '\n",
    "       151 =  'ARMENIA'\n",
    "       532 =  'ARUBA'\n",
    "       438 =  'AUSTRALIA'\n",
    "       103 =  'AUSTRIA'\n",
    "       152 =  'AZERBAIJAN'\n",
    "       512 =  'BAHAMAS'\n",
    "       298 =  'BAHRAIN'\n",
    "       274 =  'BANGLADESH'\n",
    "       513 =  'BARBADOS'\n",
    "       104 =  'BELGIUM'\n",
    "       581 =  'BELIZE'\n",
    "       386 =  'BENIN'\n",
    "       509 =  'BERMUDA'\n",
    "       153 =  'BELARUS'\n",
    "       242 =  'BHUTAN'\n",
    "       688 =  'BOLIVIA'\n",
    "       717 =  'BONAIRE, ST EUSTATIUS, SABA' \n",
    "       164 =  'BOSNIA-HERZEGOVINA'\n",
    "       336 =  'BOTSWANA'\n",
    "       689 =  'BRAZIL'\n",
    "       525 =  'BRITISH VIRGIN ISLANDS'\n",
    "       217 =  'BRUNEI'\n",
    "       105 =  'BULGARIA'\n",
    "       393 =  'BURKINA FASO'\n",
    "       243 =  'BURMA'\n",
    "       375 =  'BURUNDI'\n",
    "       310 =  'CAMEROON'\n",
    "       326 =  'CAPE VERDE'\n",
    "       526 =  'CAYMAN ISLANDS'\n",
    "       383 =  'CENTRAL AFRICAN REPUBLIC'\n",
    "       384 =  'CHAD'\n",
    "       690 =  'CHILE'\n",
    "       245 =  'CHINA, PRC'\n",
    "       721 =  'CURACAO' \n",
    "       270 =  'CHRISTMAS ISLAND'\n",
    "       271 =  'COCOS ISLANDS'\n",
    "       691 =  'COLOMBIA'\n",
    "       317 =  'COMOROS'\n",
    "       385 =  'CONGO'\n",
    "       467 =  'COOK ISLANDS'\n",
    "       575 =  'COSTA RICA'\n",
    "       165 =  'CROATIA'\n",
    "       584 =  'CUBA'\n",
    "       218 =  'CYPRUS'\n",
    "       140 =  'CZECH REPUBLIC'\n",
    "       723 =  'FAROE ISLANDS (PART OF DENMARK)'  \n",
    "       108 =  'DENMARK'\n",
    "       322 =  'DJIBOUTI'\n",
    "       519 =  'DOMINICA'\n",
    "       585 =  'DOMINICAN REPUBLIC'\n",
    "       240 =  'EAST TIMOR'\n",
    "       692 =  'ECUADOR'\n",
    "       368 =  'EGYPT'\n",
    "       576 =  'EL SALVADOR'\n",
    "       399 =  'EQUATORIAL GUINEA'\n",
    "       372 =  'ERITREA'\n",
    "       109 =  'ESTONIA'\n",
    "       369 =  'ETHIOPIA'\n",
    "       604 =  'FALKLAND ISLANDS'\n",
    "       413 =  'FIJI'\n",
    "       110 =  'FINLAND'\n",
    "       111 =  'FRANCE'\n",
    "       601 =  'FRENCH GUIANA'\n",
    "       411 =  'FRENCH POLYNESIA'\n",
    "       387 =  'GABON'\n",
    "       338 =  'GAMBIA'\n",
    "       758 =  'GAZA STRIP' \n",
    "       154 =  'GEORGIA'\n",
    "       112 =  'GERMANY'\n",
    "       339 =  'GHANA'\n",
    "       143 =  'GIBRALTAR'\n",
    "       113 =  'GREECE'\n",
    "       520 =  'GRENADA'\n",
    "       507 =  'GUADELOUPE'\n",
    "       577 =  'GUATEMALA'\n",
    "       382 =  'GUINEA'\n",
    "       327 =  'GUINEA-BISSAU'\n",
    "       603 =  'GUYANA'\n",
    "       586 =  'HAITI'\n",
    "       726 =  'HEARD AND MCDONALD IS.'\n",
    "       149 =  'HOLY SEE/VATICAN'\n",
    "       528 =  'HONDURAS'\n",
    "       206 =  'HONG KONG'\n",
    "       114 =  'HUNGARY'\n",
    "       115 =  'ICELAND'\n",
    "       213 =  'INDIA'\n",
    "       759 =  'INDIAN OCEAN AREAS (FRENCH)' \n",
    "       729 =  'INDIAN OCEAN TERRITORY' \n",
    "       204 =  'INDONESIA'\n",
    "       249 =  'IRAN'\n",
    "       250 =  'IRAQ'\n",
    "       116 =  'IRELAND'\n",
    "       251 =  'ISRAEL'\n",
    "       117 =  'ITALY'\n",
    "       388 =  'IVORY COAST'\n",
    "       514 =  'JAMAICA'\n",
    "       209 =  'JAPAN'\n",
    "       253 =  'JORDAN'\n",
    "       201 =  'KAMPUCHEA'\n",
    "       155 =  'KAZAKHSTAN'\n",
    "       340 =  'KENYA'\n",
    "       414 =  'KIRIBATI'\n",
    "       732 =  'KOSOVO' \n",
    "       272 =  'KUWAIT'\n",
    "       156 =  'KYRGYZSTAN'\n",
    "       203 =  'LAOS'\n",
    "       118 =  'LATVIA'\n",
    "       255 =  'LEBANON'\n",
    "       335 =  'LESOTHO'\n",
    "       370 =  'LIBERIA'\n",
    "       381 =  'LIBYA'\n",
    "       119 =  'LIECHTENSTEIN'\n",
    "       120 =  'LITHUANIA'\n",
    "       121 =  'LUXEMBOURG'\n",
    "       214 =  'MACAU'\n",
    "       167 =  'MACEDONIA'\n",
    "       320 =  'MADAGASCAR'\n",
    "       345 =  'MALAWI'\n",
    "       273 =  'MALAYSIA'\n",
    "       220 =  'MALDIVES'\n",
    "       392 =  'MALI'\n",
    "       145 =  'MALTA'\n",
    "       472 =  'MARSHALL ISLANDS'\n",
    "       511 =  'MARTINIQUE'\n",
    "       389 =  'MAURITANIA'\n",
    "       342 =  'MAURITIUS'\n",
    "       760 =  'MAYOTTE (AFRICA - FRENCH)' \n",
    "       473 =  'MICRONESIA, FED. STATES OF'\n",
    "       157 =  'MOLDOVA'\n",
    "       122 =  'MONACO'\n",
    "       299 =  'MONGOLIA'\n",
    "       735 =  'MONTENEGRO' \n",
    "       521 =  'MONTSERRAT'\n",
    "       332 =  'MOROCCO'\n",
    "       329 =  'MOZAMBIQUE'\n",
    "       371 =  'NAMIBIA'\n",
    "       440 =  'NAURU'\n",
    "       257 =  'NEPAL'\n",
    "       123 =  'NETHERLANDS'\n",
    "       508 =  'NETHERLANDS ANTILLES'\n",
    "       409 =  'NEW CALEDONIA'\n",
    "       464 =  'NEW ZEALAND'\n",
    "       579 =  'NICARAGUA'\n",
    "       390 =  'NIGER'\n",
    "       343 =  'NIGERIA'\n",
    "       470 =  'NIUE'\n",
    "       275 =  'NORTH KOREA'\n",
    "       124 =  'NORWAY'\n",
    "       256 =  'OMAN'\n",
    "       258 =  'PAKISTAN'\n",
    "       474 =  'PALAU'\n",
    "       743 =  'PALESTINE' \n",
    "       504 =  'PANAMA'\n",
    "       441 =  'PAPUA NEW GUINEA'\n",
    "       693 =  'PARAGUAY'\n",
    "       694 =  'PERU'\n",
    "       260 =  'PHILIPPINES'\n",
    "       416 =  'PITCAIRN ISLANDS'\n",
    "       107 =  'POLAND'\n",
    "       126 =  'PORTUGAL'\n",
    "       297 =  'QATAR'\n",
    "       748 =  'REPUBLIC OF SOUTH SUDAN'\n",
    "       321 =  'REUNION'\n",
    "       127 =  'ROMANIA'\n",
    "       158 =  'RUSSIA'\n",
    "       376 =  'RWANDA'\n",
    "       128 =  'SAN MARINO'\n",
    "       330 =  'SAO TOME AND PRINCIPE'\n",
    "       261 =  'SAUDI ARABIA'\n",
    "       391 =  'SENEGAL'\n",
    "       142 =  'SERBIA AND MONTENEGRO'\n",
    "       745 =  'SERBIA' \n",
    "       347 =  'SEYCHELLES'\n",
    "       348 =  'SIERRA LEONE'\n",
    "       207 =  'SINGAPORE'\n",
    "       141 =  'SLOVAKIA'\n",
    "       166 =  'SLOVENIA'\n",
    "       412 =  'SOLOMON ISLANDS'\n",
    "       397 =  'SOMALIA'\n",
    "       373 =  'SOUTH AFRICA'\n",
    "       276 =  'SOUTH KOREA'\n",
    "       129 =  'SPAIN'\n",
    "       244 =  'SRI LANKA'\n",
    "       346 =  'ST. HELENA'\n",
    "       522 =  'ST. KITTS-NEVIS'\n",
    "       523 =  'ST. LUCIA'\n",
    "       502 =  'ST. PIERRE AND MIQUELON'\n",
    "       524 =  'ST. VINCENT-GRENADINES'\n",
    "       716 =  'SAINT BARTHELEMY' \n",
    "       736 =  'SAINT MARTIN' \n",
    "       749 =  'SAINT MAARTEN' \n",
    "       350 =  'SUDAN'\n",
    "       602 =  'SURINAME'\n",
    "       351 =  'SWAZILAND'\n",
    "       130 =  'SWEDEN'\n",
    "       131 =  'SWITZERLAND'\n",
    "       262 =  'SYRIA'\n",
    "       268 =  'TAIWAN'\n",
    "       159 =  'TAJIKISTAN'\n",
    "       353 =  'TANZANIA'\n",
    "       263 =  'THAILAND'\n",
    "       304 =  'TOGO'\n",
    "       417 =  'TONGA'\n",
    "       516 =  'TRINIDAD AND TOBAGO'\n",
    "       323 =  'TUNISIA'\n",
    "       264 =  'TURKEY'\n",
    "       161 =  'TURKMENISTAN'\n",
    "       527 =  'TURKS AND CAICOS ISLANDS'\n",
    "       420 =  'TUVALU'\n",
    "       352 =  'UGANDA'\n",
    "       162 =  'UKRAINE'\n",
    "       296 =  'UNITED ARAB EMIRATES'\n",
    "       135 =  'UNITED KINGDOM'\n",
    "       695 =  'URUGUAY'\n",
    "       163 =  'UZBEKISTAN'\n",
    "       410 =  'VANUATU'\n",
    "       696 =  'VENEZUELA'\n",
    "       266 =  'VIETNAM'\n",
    "       469 =  'WALLIS AND FUTUNA ISLANDS'\n",
    "       757 =  'WEST INDIES (FRENCH)' \n",
    "       333 =  'WESTERN SAHARA'\n",
    "       465 =  'WESTERN SAMOA'\n",
    "       216 =  'YEMEN'\n",
    "       139 =  'YUGOSLAVIA'\n",
    "       301 =  'ZAIRE'\n",
    "       344 =  'ZAMBIA'\n",
    "       315 =  'ZIMBABWE'\n",
    "       403 =  'INVALID: AMERICAN SAMOA'\n",
    "       712 =  'INVALID: ANTARCTICA' \n",
    "       700 =  'INVALID: BORN ON BOARD SHIP'\n",
    "       719 =  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)'\n",
    "       574 =  'INVALID: CANADA'\n",
    "       720 =  'INVALID: CANTON AND ENDERBURY ISLS' \n",
    "       106 =  'INVALID: CZECHOSLOVAKIA'\n",
    "       739 =  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' \n",
    "       394 =  'INVALID: FRENCH SOUTHERN AND ANTARCTIC'\n",
    "       501 =  'INVALID: GREENLAND'\n",
    "       404 =  'INVALID: GUAM'\n",
    "       730 =  'INVALID: INTERNATIONAL WATERS' \n",
    "       731 =  'INVALID: JOHNSON ISLAND' \n",
    "       471 =  'INVALID: MARIANA ISLANDS, NORTHERN'\n",
    "       737 =  'INVALID: MIDWAY ISLANDS' \n",
    "       753 =  'INVALID: MINOR OUTLYING ISLANDS - USA'\n",
    "       740 =  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' \n",
    "       710 =  'INVALID: NON-QUOTA IMMIGRANT'\n",
    "       505 =  'INVALID: PUERTO RICO'\n",
    "        0  =  'INVALID: STATELESS'\n",
    "       705 =  'INVALID: STATELESS'\n",
    "       583 =  'INVALID: UNITED STATES'\n",
    "       407 =  'INVALID: UNITED STATES'\n",
    "       999 =  'INVALID: UNKNOWN'\n",
    "       239 =  'INVALID: UNKNOWN COUNTRY'\n",
    "       134 =  'INVALID: USSR'\n",
    "       506 =  'INVALID: U.S. VIRGIN ISLANDS'\n",
    "       755 =  'INVALID: WAKE ISLAND'  \n",
    "       311 =  'Collapsed Tanzania (should not show)'\n",
    "       741 =  'Collapsed Curacao (should not show)'\n",
    "        54 =  'No Country Code (54)'\n",
    "       100 =  'No Country Code (100)'\n",
    "       187 =  'No Country Code (187)'\n",
    "       190 =  'No Country Code (190)'\n",
    "       200 =  'No Country Code (200)'\n",
    "       219 =  'No Country Code (219)'\n",
    "       238 =  'No Country Code (238)'\n",
    "       277 =  'No Country Code (277)'\n",
    "       293 =  'No Country Code (293)'\n",
    "       300 =  'No Country Code (300)'\n",
    "       319 =  'No Country Code (319)'\n",
    "       365 =  'No Country Code (365)'\n",
    "       395 =  'No Country Code (395)'\n",
    "       400 =  'No Country Code (400)'\n",
    "       485 =  'No Country Code (485)'\n",
    "       503 =  'No Country Code (503)'\n",
    "       589 =  'No Country Code (589)'\n",
    "       592 =  'No Country Code (592)'\n",
    "       791 =  'No Country Code (791)'\n",
    "       849 =  'No Country Code (849)'\n",
    "       914 =  'No Country Code (914)'\n",
    "       944 =  'No Country Code (944)'\n",
    "       996 =  'No Country Code (996)'\n",
    "     \"\"\"\n",
    "    code_to_country = {}\n",
    "    for line in code_to_country_string.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        match = re.search(r\"([\\d]+)[=\\s]+'([\\w.\\(\\)\\s:]+)'\", line)    \n",
    "        if match:\n",
    "            code = match.group(1)\n",
    "            country = match.group(2)\n",
    "            country = country.replace(\"INVALID: \", \"\")\n",
    "            code_to_country[float(code)] = country\n",
    "    udf_code_to_country = udf(lambda code : code_to_country.get(code, None))    \n",
    "    \n",
    "    valid_immigration_data = immigration_data.withColumn(\"arrival_date\", udf_date(immigration_data.arrdate)) \\\n",
    "                                            .withColumn(\"country_of_residence\", udf_code_to_country(immigration_data.i94res)) \\\n",
    "                                            .withColumnRenamed(\"i94port\", \"iata_code\") \\\n",
    "                                            .selectExpr(\"cast(i94yr as int) as arrival_year\", \n",
    "                                                        \"cast(i94mon as int) as arrival_month\",\n",
    "                                                       \"arrival_date\",\n",
    "                                                       \"iata_code\",\n",
    "                                                       \"i94addr as first_state_visited\",\n",
    "                                                       \"cast(biryear as int) as birth_year\",\n",
    "                                                        \"cast(i94bir as int) as age\",\n",
    "                                                       \"country_of_residence\",\n",
    "                                                       \"visatype as visa_type\") \n",
    "    \n",
    "    return valid_immigration_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_table = None\n",
    "for month in [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]:\n",
    "    for year in [16]:\n",
    "        immigration_staging = extract_immigration_data(month, year)\n",
    "        immigration_valid = transform_migration_data(immigration_staging)\n",
    "        if immigration_table:\n",
    "            immigration_table = immigration_table.union(immigration_valid)\n",
    "        else:\n",
    "            immigration_table = immigration_valid\n",
    "            \n",
    "immigration_table.createOrReplaceTempView(\"valid_immigration\")\n",
    "ap_data = extract_airports_data()\n",
    "ap_data.createOrReplaceTempView(\"airports\")\n",
    "enriched_immigration_table = spark.sql(\"\"\"\n",
    "    SELECT valid_immigration.*, airports.municipality as airport_municipality\n",
    "    FROM valid_immigration\n",
    "    JOIN airports ON (airports.iata_code = valid_immigration.iata_code)        \n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_immigration_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+---------+-------------------+----------+---+--------------------+---------+--------------------+\n",
      "|arrival_year|arrival_month|arrival_date|iata_code|first_state_visited|birth_year|age|country_of_residence|visa_type|airport_municipality|\n",
      "+------------+-------------+------------+---------+-------------------+----------+---+--------------------+---------+--------------------+\n",
      "|        2016|            1|  2016-01-12|      BOS|                 MA|      1996| 20|             ALBANIA|       F1|              Boston|\n",
      "|        2016|            1|  2016-01-12|      BOS|                 MA|      1996| 20|             ALBANIA|       F1|              Boston|\n",
      "|        2016|            1|  2016-01-16|      BOS|                 CT|      1999| 17|             ALBANIA|       B2|              Boston|\n",
      "|        2016|            1|  2016-01-16|      BOS|                 CT|      1971| 45|             ALBANIA|       B2|              Boston|\n",
      "|        2016|            1|  2016-01-16|      BOS|                 CT|      2004| 12|             ALBANIA|       B2|              Boston|\n",
      "+------------+-------------+------------+---------+-------------------+----------+---+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_immigration_table.write.parquet(path=\"us_visitors.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks\n",
    "Two types of quality checks are performed for each of the tables created at the end of the ETL process.\n",
    "1. Does the table have desired schema? \n",
    "2. Does the table have non-zero number of rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_table = spark.read.parquet(\"temperatures.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_table.count() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- avg_temperature: float (nullable = true)\n",
      " |-- avg_temperature_uncertainty: float (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_schema_test = [(\"city\", StringType()),\n",
    "                           (\"country\", StringType()),\n",
    "                           (\"latitude\", StringType()),\n",
    "                           (\"longitude\", StringType()),\n",
    "                           (\"avg_temperature\", FloatType()),\n",
    "                           (\"avg_temperature_uncertainty\", FloatType()),\n",
    "                           (\"date\", DateType()),\n",
    "                           (\"month\", IntegerType()),\n",
    "                           (\"year\", IntegerType())]\n",
    "for test in temperatures_schema_test:\n",
    "    column = test[0]\n",
    "    expected = test[1]\n",
    "    if temperatures_table.schema[column].dataType != expected:\n",
    "        raise ValueError(\"temperatures.{} must be of type {}\".format(column, expected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_table = spark.read.parquet(\"airports_in_usa.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|iata_code| municipality|\n",
      "+---------+-------------+\n",
      "|      OCA|    Key Largo|\n",
      "|      PQS|Pilot Station|\n",
      "|      CSE|Crested Butte|\n",
      "|      JCY| Johnson City|\n",
      "|      PMX|       Palmer|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if airports_table.count() == 0:\n",
    "    raise ValueError(\"airports_in_usa is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_schema_test = [(\"municipality\", StringType()),\n",
    "                        (\"iata_code\", StringType())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in airports_schema_test:\n",
    "    column = test[0]\n",
    "    expected = test[1]\n",
    "    if airports_table.schema[column].dataType != expected:\n",
    "        raise ValueError(\"airports_in_usa.{} must be of type {}\".format(column, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_visitors_table = spark.read.parquet(\"us_visitors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if us_visitors_table.count() == 0:\n",
    "    raise ValueError(\"Migrations table is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- first_state_visited: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country_of_residence: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- airport_municipality: string (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_visitors_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_visitors_schema_test = [(\"arrival_date\", DateType()),\n",
    "                            (\"iata_code\", StringType()),\n",
    "                            (\"first_state_visited\", StringType()),\n",
    "                            (\"birth_year\", IntegerType()),\n",
    "                            (\"age\", IntegerType()),\n",
    "                            (\"country_of_residence\", StringType()),\n",
    "                            (\"arrival_year\", IntegerType()),\n",
    "                            (\"arrival_month\", IntegerType()),\n",
    "                            (\"visa_type\", StringType()),\n",
    "                            (\"airport_municipality\", StringType())]\n",
    "\n",
    "for test in us_visitors_schema_test:\n",
    "    column = test[0]\n",
    "    expected = test[1]\n",
    "    if us_visitors_table.schema[column].dataType != expected:\n",
    "        raise ValueError(\"us_visitors.{} must be of type {}\".format(column, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_table = spark.read.parquet(\"us_demographics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if demographics_table.count() == 0:\n",
    "    raise ValueError(\"us_demographics table is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- male_population: long (nullable = true)\n",
      " |-- female_population: long (nullable = true)\n",
      " |-- total_population: long (nullable = true)\n",
      " |-- foreign_born: long (nullable = true)\n",
      " |-- avg_household_size: integer (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_table.printSchema()\n",
    "us_demographics_schema_test = [(\"city\", StringType()),\n",
    "                               (\"state\", StringType()),\n",
    "                               (\"state_code\", StringType()),\n",
    "                               (\"male_population\", LongType()),\n",
    "                               (\"female_population\", LongType()),\n",
    "                               (\"total_population\", LongType()),\n",
    "                               (\"foreign_born\", LongType()),\n",
    "                               (\"avg_household_size\", IntegerType()),\n",
    "                               (\"median_age\", FloatType()),\n",
    "                               (\"race\", StringType())]\n",
    "\n",
    "for test in us_demographics_schema_test:\n",
    "    column = test[0]\n",
    "    expected = test[1]\n",
    "    if demographics_table.schema[column].dataType != expected:\n",
    "        raise ValueError(\"demographics.{} must be of type {}\".format(column, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized in four tables.\n",
    "Dimension tables: airports_in_usa, us_demographics\n",
    "Fact tables: temperatures, migrations\n",
    "\n",
    "##### airports_in_usa\n",
    "This table contains airports with IATA code and municipality name where it is located.\n",
    "More information about the original dataset of airport codes is available here https://datahub.io/core/airport-codes#readme\n",
    "\n",
    "Columns:\n",
    "* iata_code (string): IATA Code of the airport\n",
    "* municipality (string): name of jurisdiction where the airport is located\n",
    "\n",
    "##### us_demographics\n",
    "This table contains information about the population demographics of cities in the USA based on Census survey data of 2015.\n",
    "More information about the original source of dataset can be found here: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/\n",
    "\n",
    "Columns:\n",
    "* state (string): Name of state of USA\n",
    "* state_code (string): two letter abbreviated state code for the state\n",
    "* male_population (long): population count of males\n",
    "* female_population (long): population count of females\n",
    "* total_population (long): total population of the city\n",
    "* foreign_born (long): population count of foreign born residents\n",
    "* avg_household_size (integer): average household size\n",
    "* median_age (float): median age of people\n",
    "* race (string): race of the people\n",
    "\n",
    "##### temperatures\n",
    "This table contains information on average temperature of cities around the world on each day measured from years 1750 to 2013.\n",
    "More information about the original data source is here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "(\"city\", StringType()),\n",
    "                           (\"country\", StringType()),\n",
    "                           (\"latitude\", StringType()),\n",
    "                           (\"longitude\", StringType()),\n",
    "                           (\"avg_temperature\", FloatType()),\n",
    "                           (\"avg_temperature_uncertainty\", FloatType()),\n",
    "                           (\"date\", DateType()),\n",
    "                           (\"month\", IntegerType()),\n",
    "                           (\"year\", IntegerType())]\n",
    "Columns:\n",
    "* city (string): name of city\n",
    "* country (string): name of country\n",
    "* latitude (string): latitude of the city\n",
    "* longitude (string): longitude of the city\n",
    "* avg_temperature (float): average temperature of the day\n",
    "* avg_temperature_uncertainty (float):  the 95% confidence interval around the average temperature\n",
    "* date (date): date when the reading of temperature was taken\n",
    "* month (integer): month of the temperature reading\n",
    "* year (integer): year of the temperature reading\n",
    "\n",
    "##### us_visitors\n",
    "This table contains information about the details of immigrants to USA at various ports during 2016.\n",
    "More details about the original data soruce: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "Columns:\n",
    "* arrival_date (date): date of arrival of visitor\n",
    "* arrival_year (integer): year of arrival of visitor\n",
    "* arrival_month (integer): month of arrival of visitor\n",
    "* country_of_residence (string): country of residence of visitor\n",
    "* iata_code (string): IATA code of airport where the visitor arrived\n",
    "* first_state_visited (string): two letter abbreviation of state code to be first visited by visitor\n",
    "* birth_year (integer): birth year of visitor\n",
    "* age (integer): age of visitor on arrival to USA\n",
    "* visa_type (string): type of visa of visitor\n",
    "* airport_municipality (string): jurisdiction where airport is located\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I chose to work with Apache Spark for performing the ETL tasks. Spark's dataframe API provides very convenient way to wrangle data using powerful SQL like operations. Once I extracted data from the downloaded datasets using Spark and performed necessary transformations, I saved the final tables of data model in Parquet file format. This allows the tables to be retrieved in future while retaining the datatype and schema assigned to the columns of the tables during ETL process.\n",
    "\n",
    "To continue gaining insights on regular basis from this dataset, it is important to keep the data stored in the tables updated. The frequency of updating this table is dependent on the frequency at which the underlying source datasets are updated.\n",
    "\n",
    "**temperatures table**\n",
    "The data is loaded from original dataset containing average temperature of the cities around the year. Considering the frequency of the data entries in this dataset being on day level, this dataset should be ideally updated on daily basis if the the required data is available from organization like NASA.\n",
    "\n",
    "**us_visitors table:**\n",
    "This table is loaded from the the dataset about the visitors to various airports in USA around the year. As the records in this original source dataset have granuarity of day level, this table should be ideally updated on daily basis if the updates to original dataset is available on daily basis.\n",
    "\n",
    "**airports_in_usa table:**\n",
    "It depends on the dataset about airport codes and locations of airports around the world. This data is supposed to be changing less frequently. Updating it once per month should be sufficient.\n",
    "\n",
    "**us_demographics table:**\n",
    "Data about the population of different genders and races across different cities around USA. This data is derived from Census survey data that is carried out once a year on average. Therefore, this dataset should be updated once per year.\n",
    "\n",
    "If the dataset underlying this project would have increased by 100x, it would mean that the RAM installed on one commodity hardware won't be sufficient to process it desirable timeframe. In such scenario, it is preferable to run the pipeline on Apache Spark cluster running over 10s of compute nodes to parallelize the task across the RAM of multiple machines.\n",
    "\n",
    "If the dataset is utilized for populating a dashboard and is supposed to be updated on daily basis by 7am every day, then I would set up the data pipeline using Apache Airflow. There I would create an DAG (Directed Acyclic Graph) that performs inter-dependent tasks like downloading fresh datasets from web, extracting data from those downloaded files into Apache Spark cluster, perform data wrangling on the dataframes in Spark to do necessary transformations on data, combine the datasets and load the designated tables in the schema. I would set the schedule interval to be `daily` and start time to be around 5am for the DAG in Airflow so that the data is updated by 7am after necessary retries in case of failures.\n",
    "\n",
    "If the database was need to be accessed by let's say 100+ people in a company's environment, I would host the data tables into a data warehouse solution like Amazon Redshift cluster. It's columnar based storage system would allow faster processing of aggregations over different attribute of the Fact tables simultaneously by large number of users. I may also allocate more nodes and instance size to the cluster to serve the required performance to answer the analytical queries raised by people in reasonable response time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
